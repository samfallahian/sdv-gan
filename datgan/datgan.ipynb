{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:23:40.522259Z",
     "end_time": "2023-04-21T23:23:44.422530Z"
    }
   },
   "outputs": [],
   "source": [
    "import datgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package datgan:\n",
      "\n",
      "NAME\n",
      "    datgan - Top-level package for DATGAN\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    datgan\n",
      "    evaluation (package)\n",
      "    synthesizer (package)\n",
      "    utils (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        datgan.datgan.DATGAN\n",
      "    \n",
      "    class DATGAN(builtins.object)\n",
      "     |  DATGAN(loss_function=None, label_smoothing='TS', output='output', gpu=None, num_epochs=100, batch_size=500, save_checkpoints=True, restore_session=True, learning_rate=None, g_period=None, l2_reg=None, z_dim=200, num_gen_rnn=100, num_gen_hidden=50, num_dis_layers=1, num_dis_hidden=100, noise=0.5, conditional_inputs=None, verbose=1)\n",
      "     |  \n",
      "     |  The DATGAN is a synthesizer for tabular data. It uses LSTM cells to generate synthetic data for continuous and\n",
      "     |  categorical variable types. In addition, a Directed Acyclic Graph (DAG) can be provided to represent the structure\n",
      "     |  between the variables and help the model to perform better. This model integrates two types of conditionality:\n",
      "     |  rejection by sampling and conditional inputs.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss_function=None, label_smoothing='TS', output='output', gpu=None, num_epochs=100, batch_size=500, save_checkpoints=True, restore_session=True, learning_rate=None, g_period=None, l2_reg=None, z_dim=200, num_gen_rnn=100, num_gen_hidden=50, num_dis_layers=1, num_dis_hidden=100, noise=0.5, conditional_inputs=None, verbose=1)\n",
      "     |      Constructs all the necessary attributes for the DATGAN class.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      loss_function: str, default None\n",
      "     |          Name of the loss function to be used. If not specified, it will choose between 'WGAN' and 'WGGP'\n",
      "     |          depending on the ratio of continuous and categorical columns. Only accepts the values 'SGAN', 'WGAN',\n",
      "     |          and 'WGGP'.\n",
      "     |      label_smoothing: str, default 'TS'\n",
      "     |          Type of label smoothing. Only accepts the values 'TS', 'OS', and 'NO'.\n",
      "     |      output: str, default 'output'\n",
      "     |          Path to store the model and its artifacts.\n",
      "     |      gpu: int, default None\n",
      "     |          Model will automatically try to use GPU if tensorflow can use CUDA. However, this parameter allows you\n",
      "     |          to choose which GPU you want to use.\n",
      "     |      num_epochs: int, default 100\n",
      "     |          Number of epochs to use during training.\n",
      "     |      batch_size: int, default 500\n",
      "     |          Size of the batch to feed the model at each step.\n",
      "     |      save_checkpoints: bool, default True\n",
      "     |          Whether to store checkpoints of the model after each training epoch.\n",
      "     |      restore_session: bool, default True\n",
      "     |          Whether continue training from the last checkpoint.\n",
      "     |      learning_rate: float, default None\n",
      "     |          Learning rate. If set to None, the value will be set according to the chosen loss function.\n",
      "     |      g_period: int, default None\n",
      "     |          Every \"g_period\" steps, train the generator once. (Used to train the discriminator more than the\n",
      "     |          generator) By default, it will choose values according the chosen loss function.\n",
      "     |      l2_reg: bool, default None\n",
      "     |          Tell the model to use L2 regularization while training both NNs. By default, it applies the L2\n",
      "     |          regularization when using the SGAN loss function.\n",
      "     |      z_dim: int, default 200\n",
      "     |          Dimension of the noise vector used as an input to the generator.\n",
      "     |      num_gen_rnn: int, default 100\n",
      "     |          Size of the hidden units in the LSTM cell.\n",
      "     |      num_gen_hidden: int, default 50\n",
      "     |          Size of the hidden layer used on the output of the generator to act as a convolution.\n",
      "     |      num_dis_layers: int, default 1\n",
      "     |          Number of layers for the discriminator.\n",
      "     |      num_dis_hidden: int, default 100\n",
      "     |          Size of the hidden layers in the discriminator.\n",
      "     |      noise: float, default 0.2\n",
      "     |          Upper bound to the gaussian noise added to with the label smoothing. (only used if label_smoothing is\n",
      "     |          set to 'TS' or 'OS')\n",
      "     |      conditional_inputs: list, default None\n",
      "     |          List of variables in the dataset that are used as conditional inputs to the model.\n",
      "     |      verbose: int, default 0\n",
      "     |          Level of verbose. 0 means nothing, 1 means that some details will be printed, 2 is mostly used for\n",
      "     |          debugging purpose.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If the parameter loss_function is not correctly defined.\n",
      "     |      ValueError\n",
      "     |          If the parameter label_smoothing is not correctly defined.\n",
      "     |  \n",
      "     |  fit(self, data, metadata=None, dag=None, preprocessed_data_path=None)\n",
      "     |      Fit the DATGAN model to the original encoded data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data: pandas.DataFrame\n",
      "     |          Original dataset\n",
      "     |      metadata: dict, default None\n",
      "     |          Dictionary containing information about the data in the dataframe\n",
      "     |      dag: networkx.DiGraph, default None\n",
      "     |          Directed Acyclic Graph representing the relations between the variables. If no dag is provided, the\n",
      "     |          algorithm will create a linear DAG.\n",
      "     |      preprocessed_data_path: str, default None\n",
      "     |          Path to an existing preprocessor. If None is given, the model will preprocess the data and save it under\n",
      "     |          self.output + '/encoded_data'.\n",
      "     |  \n",
      "     |  load(self, data, dag=None, preprocessed_data_path=None)\n",
      "     |      Load the model based on the latest checkpoint\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data: pandas.DataFrame\n",
      "     |          Original dataset\n",
      "     |      dag: networkx.DiGraph, default None\n",
      "     |          Directed Acyclic Graph representing the relations between the variables. If no dag is provided, the\n",
      "     |          algorithm will create a linear DAG.\n",
      "     |      preprocessed_data_path: str, default None\n",
      "     |          Path to an existing preprocessor. If None is given, the model will preprocess the data and save it under\n",
      "     |          self.output + '/encoded_data'.\n",
      "     |  \n",
      "     |  preprocess(self, data, metadata, preprocessed_data_path=None)\n",
      "     |      Preprocess the original data to transform it into a usable dataset for the DATGAN model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data: pandas.DataFrame\n",
      "     |          Original dataset\n",
      "     |      metadata: dict\n",
      "     |          Dictionary containing information about the data in the dataframe\n",
      "     |      preprocessed_data_path: str, default None\n",
      "     |          Path to an existing preprocessor. If None is given, the model will preprocess the data and save it under\n",
      "     |          self.output + '/encoded_data'.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      FileNotFoundError\n",
      "     |          If the files 'preprocessed_data.pkl' and 'preprocessor.pkl' are not in the folder given in the variable\n",
      "     |          preprocessed_data_path\n",
      "     |  \n",
      "     |  sample(self, num_samples, inputs=None, cond_dict=None, sampling='SS', randomize=True, timeout=True)\n",
      "     |      Create a DataFrame with the synthetic generate data. Conditionality is done through a rejection sampling\n",
      "     |      process. For categorical variables, you need to provide the categories you want to get. For continuous\n",
      "     |      variables, you need to provide a lambda function that returns a boolean based on the values of the variable.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num_samples: int\n",
      "     |          Number of sample to provide\n",
      "     |      inputs: dict or pandas.DataFrame, default None\n",
      "     |          Optional input data. Used if the conditional inputs have been set.\n",
      "     |      cond_dict: dict, default None\n",
      "     |          Conditional dictionary.\n",
      "     |      sampling: str, default 'SS'\n",
      "     |          Type of sampling to use. Only accepts the following values: 'SS', 'SA', 'AS', and 'AA'\n",
      "     |      randomize: bool, default True\n",
      "     |          Randomize the conditional inputs if set to True. If set to False, it will not discard any samples.\n",
      "     |      timeout: bool, default True\n",
      "     |          Use a timeout to stop sampling if the model can't generate the data asked in the conditional dict\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      pandas.DataFrame:\n",
      "     |          Synthetic dataset of 'num_samples' rows\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    advise(data, dag, plot_graphs=False)\n",
      "        Give advice about which edge could be added in the DAG based on Pearson and Spearman correlations\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data: pandas.DataFrame\n",
      "            Original dataset\n",
      "        dag: networkx.DiGraph\n",
      "            Directed Acyclic Graph representing the relations between the variables\n",
      "        plot_graphs: bool\n",
      "            Whether to plot some graphs or not\n",
      "    \n",
      "    ml_assessment(df_orig, df_synth, continuous_columns, categorical_columns, ignore_cols=[], params=None)\n",
      "        Train the LightGBMCV model on all the columns of the original and the synthetic datasets.\n",
      "        \n",
      "        Coded by Tim Hillel.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        df_orig: pandas.DataFrame\n",
      "            Original dataset\n",
      "        df_synth: pandas.DataFrame\n",
      "            Synthetic dataset\n",
      "        continuous_columns: list[str]\n",
      "            List of continuous variables\n",
      "        categorical_columns: list[str]\n",
      "            List of categorical variables\n",
      "        ignore_cols: list[str], default []\n",
      "            List of columns to ignore\n",
      "        params: dict\n",
      "            Dictionary of parameters passed to the LightGBMCV model\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict:\n",
      "            Dictionary with the variable names as keys and results as value\n",
      "    \n",
      "    stats_assessment(original_data, synthetic_data, continuous_columns, aggregation_level, sep='::', ignore_cols=[])\n",
      "        Compute all the stats on the different combinations based on the aggregation_level.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        original_data: pandas.DataFrame\n",
      "            Original dataset\n",
      "        synthetic_data: pandas.DataFrame\n",
      "            Synthetic dataset\n",
      "        continuous_columns: list[str]\n",
      "            List of continuous columns\n",
      "        aggregation_level: int\n",
      "            Aggregation level for the stats. Only accepts 1, 2, or 3.\n",
      "        sep: str, default '::'\n",
      "            String used to separated the columns while aggregating them.\n",
      "        ignore_cols: list[str], default []\n",
      "            List of columns to ignore while computing the stats.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        stats: dict\n",
      "            Dictionary containing all the stats for all the combinations\n",
      "    \n",
      "    transform_results(results, continuous_columns, categorical_columns, ignore_cols=[])\n",
      "        Transform the results of the function `ml_assessment` in human-readable format.\n",
      "        \n",
      "        Coded by Tim Hillel.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        results: dict\n",
      "            Dictionary of results from the `ml_assessment` function. Keys corresponds to synthetic files testes, value to\n",
      "            the dictionary returned by the `ml_assessment` function.\n",
      "        continuous_columns: list[str]\n",
      "            List of continuous variables\n",
      "        categorical_columns: list[str]\n",
      "            List of categorical variables\n",
      "        ignore_cols: list[str], default []\n",
      "            List of columns to ignore\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "            cont_sorted: list[tuple]\n",
      "                Synthetic dataset sorted based on the results on continuous columns\n",
      "            cat_sorted: list[tuple]\n",
      "                Synthetic dataset sorted based on the results on categorical columns\n",
      "\n",
      "DATA\n",
      "    __all__ = {'DATGAN', 'advise', 'ml_assessment', 'stats_assessment', 't...\n",
      "    __email__ = 'gael.lederrey@epfl.ch'\n",
      "\n",
      "VERSION\n",
      "    2.1.10\n",
      "\n",
      "AUTHOR\n",
      "    Gael Lederrey\n",
      "\n",
      "FILE\n",
      "    c:\\users\\hesam-pc\\miniconda3\\envs\\datgan\\lib\\site-packages\\datgan\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datgan)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:24:24.934500Z",
     "end_time": "2023-04-21T23:24:24.970405Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:24:47.587232Z",
     "end_time": "2023-04-21T23:24:47.601178Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/wind.csv\", index_col=False)\n",
    "\n",
    "data_info = {\n",
    "    'distance': {\n",
    "        'type': 'continuous',\n",
    "        'bounds': [0.0, np.infty],\n",
    "        'discrete': False,\n",
    "    },\n",
    "    'age': {\n",
    "        'type': 'continuous',\n",
    "        'bounds': [0, 100],\n",
    "        'enforce_bounds': True,\n",
    "        'discrete': True\n",
    "    },\n",
    "    'departure_time': {\n",
    "        'type': 'continuous',\n",
    "        'bounds': [0, 23.999],\n",
    "        'discrete': False\n",
    "    }\n",
    "}\n",
    "\n",
    "# For the categorical columns, we can simply add them using a for loop\n",
    "for c in df.columns:\n",
    "    if c not in data_info.keys():\n",
    "        data_info[c] = {'type': 'categorical'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import networkx as nx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:26:13.069830Z",
     "end_time": "2023-04-21T23:26:13.088134Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = nx.DiGraph()\n",
    "graph.add_edges_from([\n",
    "    (\"age\", \"license\"),\n",
    "    (\"age\", \"education_level\"),\n",
    "    (\"gender\", \"work_status\"),\n",
    "    (\"education_level\", \"work_status\"),\n",
    "    (\"education_level\", \"hh_income\"),\n",
    "    (\"work_status\", \"hh_income\"),\n",
    "    (\"hh_income\", \"hh_descr\"),\n",
    "    (\"hh_income\", \"hh_size\"),\n",
    "    (\"hh_size\", \"hh_vehicles\"),\n",
    "    (\"hh_size\", \"hh_bikes\"),\n",
    "    (\"work_status\", \"trip_purpose\"),\n",
    "    (\"trip_purpose\", \"departure_time\"),\n",
    "    (\"trip_purpose\", \"distance\"),\n",
    "    (\"travel_dow\", \"choice\"),\n",
    "    (\"distance\", \"choice\"),\n",
    "    (\"departure_time\", \"choice\"),\n",
    "    (\"hh_vehicles\", \"choice\"),\n",
    "    (\"hh_bikes\", \"choice\"),\n",
    "    (\"license\", \"choice\"),\n",
    "    (\"education_level\", \"hh_size\"),\n",
    "    (\"work_status\", \"hh_descr\"),\n",
    "    (\"work_status\", \"hh_size\"),\n",
    "    (\"hh_income\", \"hh_bikes\"),\n",
    "    (\"hh_income\", \"hh_vehicles\"),\n",
    "    (\"trip_purpose\", \"choice\")\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
